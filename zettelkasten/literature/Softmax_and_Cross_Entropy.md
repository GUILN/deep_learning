
## Softmax Function
* Helps `output layer` to sum up to one in case of probabilistic outcomes like classification
* Is a generalization of logistics function
* Normalizes outcome values to be between 0 and 1, and makes them add up to 1

## Cross-Entropy Function

* Is an loss function
* It has advantages over MSE (mean squared error)
* Is the preferred method `only` for classification

## Video to Watch
- [ ] Google - Softmax output function - Jefrey Hinton

## Papers to Read
- [ ] A Friendly introduction to Cross-Entropy Loss - Rob DiPietro - 2016
- [ ] How to implement a neural network intermezzo 2 - Peter Roelants - 2016